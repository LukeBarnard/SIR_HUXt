{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.time import Time, TimeDelta\n",
    "import astropy.units as u\n",
    "import glob\n",
    "import h5py\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sunpy.coordinates.sun as sn\n",
    "import scipy.ndimage as ndi\n",
    "import scipy.stats as st\n",
    "# Our own library for using spice with STEREO (https://github.com/LukeBarnard/stereo_spice)\n",
    "from stereo_spice.coordinates import StereoSpice\n",
    "# Local packages\n",
    "import HUXt as H\n",
    "\n",
    "spice = StereoSpice()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_huxt(start_time, wind='uniform'):\n",
    "    \"\"\"\n",
    "    Initialise HUXt with some predetermined boundary/initial conditions\n",
    "    start_time should be astropy.Time object.\n",
    "    wind should be uniform or structured\n",
    "    \"\"\"\n",
    "    cr_num = np.fix(sn.carrington_rotation_number(start_time))\n",
    "    ert = H.Observer('EARTH', start_time)\n",
    "\n",
    "    # Set up HUXt for a 5 day simulation with homogenous inner boundary.\n",
    "    vr_in, br_in = H.Hin.get_MAS_long_profile(cr_num, ert.lat.to(u.deg))\n",
    "    if wind == 'uniform':\n",
    "        vr_in = np.zeros(vr_in.shape) + 400*vr_in.unit\n",
    "        \n",
    "    model = H.HUXt(v_boundary=vr_in, cr_num=cr_num, cr_lon_init=ert.lon_c, latitude=ert.lat.to(u.deg),\n",
    "                   br_boundary=br_in, lon_start=270*u.deg, lon_stop=90*u.deg, simtime=3.5*u.day, dt_scale=4)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_base_cme(v=1000, lon=0, lat=0, width=45, thickness=5):\n",
    "    \"\"\"\n",
    "    Return the base CME, which is used to establish the pseudo-truth CME and the SIR ensemble\n",
    "    \"\"\"\n",
    "    t_launch = (1*u.hr).to(u.s)\n",
    "    cme = H.ConeCME(t_launch=t_launch, longitude=lon*u.deg, latitude=lat*u.deg, width=width*u.deg, v=v*(u.km/u.s), thickness=thickness*u.solRad)\n",
    "    return cme\n",
    "\n",
    "def perturb_cone_cme(cme):\n",
    "    \"\"\"\n",
    "    Perturb a ConeCME's parameters. Used to establish the pseudo-truth CME and the initial SIR ensemble members. \n",
    "    \"\"\"\n",
    "    lon_new = perturb_cme_param(cme.longitude, 15*u.deg)\n",
    "    lat_new = perturb_cme_param(cme.latitude, 15*u.deg)\n",
    "    width_new = perturb_cme_param(cme.width, 15*u.deg)\n",
    "    speed_new = perturb_cme_param(cme.v, 150*u.km/u.s)\n",
    "    thickness_new = perturb_cme_param(cme.thickness, 1*u.solRad)\n",
    "\n",
    "    cme_perturb = H.ConeCME(t_launch=cme.t_launch,\n",
    "                            longitude=lon_new,\n",
    "                            latitude=lat_new,\n",
    "                            width=width_new,\n",
    "                            v=speed_new,\n",
    "                            thickness=thickness_new)\n",
    "    return cme_perturb\n",
    "\n",
    "def perturb_cme_param(param, spread):\n",
    "    \"\"\"\n",
    "    Randomly perturb a CME paramter with the uniform distribution and a specified spread\n",
    "    \"\"\"\n",
    "    param_new = param + np.random.uniform(-1, 1, 1)[0] * spread\n",
    "    return param_new\n",
    "\n",
    "def compute_observed_cme_profile(observer_lon, cme, dt_scale=5, el_max=30):\n",
    "    \"\"\"\n",
    "    Compute the time-elongation profile of a specified observer.\n",
    "    Observer longitude is relative to Earth.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute the time-elongation profiles of the CME flanks from STA and STB\n",
    "    coords = compute_t_e_profile(observer_lon, cme)\n",
    "\n",
    "    # Remove invalid points\n",
    "    coords.dropna(inplace=True)\n",
    "\n",
    "    # Add observation noise.\n",
    "    obs = coords.loc[:, ['time', 'el']].copy()\n",
    "    obs['el'] = obs['el'] + 0.5*np.random.randn(obs.shape[0])\n",
    "\n",
    "    # Only keep every dt_scale'th observation and reindex - dt_scale=5 corrsponds to ~2hr\n",
    "    obs = obs[::dt_scale]\n",
    "    obs.set_index(np.arange(0, obs.shape[0]), inplace=True)\n",
    "\n",
    "    # Only analyse up to 30 deg elon ~ approx the HI1 FOV\n",
    "    id_fov = obs['el'] <= el_max\n",
    "    obs = obs[id_fov]\n",
    "    return obs\n",
    "    \n",
    "def compute_t_e_profile(observer_lon, cme):\n",
    "    \"\"\"\n",
    "    Compute the time elongation profile of the flank of a ConeCME in HUXt. The observer longtidue is specified relative to Earth,\n",
    "    and but otherwise matches Earth's coords. \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    observer_lon: Angular separation of Earth and the observer, in HEEQ.\n",
    "    cme: A ConeCME object from a completed HUXt run (i.e the ConeCME.coords dictionary has been populated).\n",
    "    Returns\n",
    "    -------\n",
    "    obs_profile: Pandas dataframe giving the coordinates of the ConeCME flank from STA's perspective, including the\n",
    "                time, elongation, position angle, and HEEQ radius and longitude.\n",
    "    \"\"\"\n",
    "    times = Time([coord['time'] for i, coord in cme.coords.items()])\n",
    "    \n",
    "    # Compute observers location using earth ephem, adding on observers longitude offset from Earth and correct for runover 2*pi\n",
    "    obs = H.Observer('EARTH', times)\n",
    "    obs.lon = obs.lon + observer_lon\n",
    "    id_over = obs.lon > 2*np.pi*u.rad\n",
    "    if np.any(id_over):\n",
    "        obs.lon[id_over] = obs.lon[id_over] - 2*np.pi*u.rad\n",
    "    \n",
    "    obs_profile = pd.DataFrame(index=np.arange(times.size), columns=['time', 'el', 'r', 'lon'])\n",
    "    obs_profile['time'] = times.jd\n",
    "    \n",
    "    for i, coord in cme.coords.items():\n",
    "\n",
    "        if len(coord['r']) == 0:\n",
    "            obs_profile.loc[i, ['lon','r', 'el']] = np.NaN\n",
    "            continue\n",
    "\n",
    "        r_obs = obs.r[i]\n",
    "        x_obs = obs.r[i] * np.cos(obs.lat[i]) * np.cos(obs.lon[i])\n",
    "        y_obs = obs.r[i] * np.cos(obs.lat[i]) * np.sin(obs.lon[i])\n",
    "        z_obs = obs.r[i] * np.sin(obs.lat[i])\n",
    "\n",
    "        lon_cme = coord['lon']\n",
    "        lat_cme = coord['lat']\n",
    "        r_cme = coord['r']\n",
    "\n",
    "        x_cme = r_cme * np.cos(lat_cme) * np.cos(lon_cme)\n",
    "        y_cme = r_cme * np.cos(lat_cme) * np.sin(lon_cme)\n",
    "        z_cme = r_cme * np.sin(lat_cme)\n",
    "        #############\n",
    "        # Compute the observer CME distance, S, and elongation\n",
    "\n",
    "        x_cme_s = x_cme - x_obs\n",
    "        y_cme_s = y_cme - y_obs\n",
    "        z_cme_s = z_cme - z_obs\n",
    "        s = np.sqrt(x_cme_s**2 + y_cme_s**2 + z_cme_s**2)\n",
    "\n",
    "        numer = (r_obs**2 + s**2 - r_cme**2).value\n",
    "        denom = (2.0 * r_obs * s).value\n",
    "        e_obs = np.arccos(numer / denom)\n",
    "\n",
    "        # Find the flank coordinate and update output\n",
    "        id_obs_flank = np.argmax(e_obs)       \n",
    "        obs_profile.loc[i, 'lon'] = lon_cme[id_obs_flank].value\n",
    "        obs_profile.loc[i, 'r'] = r_cme[id_obs_flank].value\n",
    "        obs_profile.loc[i, 'el'] = np.rad2deg(e_obs[id_obs_flank])\n",
    "    \n",
    "    # Force values to be floats.\n",
    "    keys = ['lon', 'r', 'el']\n",
    "    obs_profile[keys] = obs_profile[keys].astype(np.float64)\n",
    "    return obs_profile\n",
    "\n",
    "def twin_experiment():\n",
    "    \n",
    "    start_time = Time('2008-01-01T00:00:00')\n",
    "    model = setup_huxt(start_time)\n",
    "    \n",
    "    cme_base = get_base_cme()\n",
    "    \n",
    "    cme_truth = perturb_cone_cme(cme_base)\n",
    "\n",
    "    model.solve([cme_truth])\n",
    "    \n",
    "    cme_truth = model.cmes[0]\n",
    "    obs_lon = -30*u.deg\n",
    "    obs_profile = compute_observed_cme_profile(obs_lon, cme_truth)\n",
    "    \n",
    "def compute_observation_likelihood(t_obs, e_obs, profile):\n",
    "    \"\"\"\n",
    "    Compute the likelihood of an observed elongation measurement for a modelled elongation measurement.\n",
    "    Assumes a gaussian likelihood centered on the modelled elongation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Find the modelled elon at observation time\n",
    "    e_mod = profile.loc[profile['time'] == t_obs, 'el'].values[0]\n",
    "    # Compute likelihood of the observation.\n",
    "    if np.isnan(e_mod):\n",
    "        lkhd = np.NaN\n",
    "    else:\n",
    "        lkhd = st.norm.pdf(e_obs, loc=e_mod, scale=2.0)\n",
    "        \n",
    "    return lkhd\n",
    "\n",
    "def get_cme_params_for_sir(cme):\n",
    "    \"\"\"\n",
    "    Form an array of the CME parameter values that are kept track of in the ensemble members.\n",
    "    \"\"\"\n",
    "    params = np.array([cme.t_launch.to('s').value, cme.longitude.to('rad').value, cme.latitude.to('rad').value,\n",
    "                          cme.width.to('rad').value, cme.v.value, cme.thickness.to('km').value])\n",
    "    \n",
    "    return params\n",
    "\n",
    "def create_analysis_output_file(filename):\n",
    "    \"\"\"\n",
    "    Create a HDF5 file for storing the SIR analysis steps.\n",
    "    \"\"\"\n",
    "    proj_dirs = H._setup_dirs_()\n",
    "    out_filepath = os.path.join(proj_dirs['out_data'], filename)\n",
    "    if os.path.isfile(out_filepath):\n",
    "        # File exists, so delete and start new.\n",
    "        print(\"Warning: {} already exists. Overwriting\".format(out_filepath))\n",
    "        os.remove(out_filepath)\n",
    "\n",
    "    out_file = h5py.File(out_filepath, 'w')\n",
    "    return out_file\n",
    "\n",
    "def cme_kde_resample_with_weights(cme_prior, weights):\n",
    "    \"\"\"\n",
    "    Use kernel density estimation to resample particles from the prior distriubtion given the particle weights.\n",
    "    \"\"\"\n",
    "    # Find valid weights, and pull out each corresponding particles parameters\n",
    "    n_ensemble = len(weights)\n",
    "    valid_weights = np.isfinite(weights)\n",
    "    weights = weights[valid_weights]\n",
    "    lon = cme_prior[valid_weights, 1].squeeze()\n",
    "    lat = cme_prior[valid_weights, 2].squeeze()\n",
    "    width = cme_prior[valid_weights, 3].squeeze()\n",
    "    v = cme_prior[valid_weights, 4].squeeze()\n",
    "    thick = cme_prior[valid_weights, 5].squeeze()\n",
    "    \n",
    "    # Force lon so no disconinuity at 2pi\n",
    "    lon[lon>=np.pi] += -2*np.pi\n",
    "\n",
    "    # KDE fit each distribution, draw random sample. \n",
    "    cme_update = np.zeros(cme_prior.shape)*np.NaN\n",
    "    cme_update[:, 0] = cme_prior[:, 0].copy()\n",
    "    \n",
    "    col_id = [1,2,3,4,5]\n",
    "    param = [lon, lat, width, v, thick]\n",
    "    for col, p in zip(col_id, param):\n",
    "        kde = st.gaussian_kde(p, bw_method='silverman',  weights=weights)\n",
    "        sample = kde.resample(n_ensemble)\n",
    "        cme_update[:, col] = sample.copy()\n",
    "        \n",
    "    # Put lon back on 0-2pi domain\n",
    "    id_low = cme_update[:, 1] < 0\n",
    "    cme_update[id_low, 1] += 2*np.pi\n",
    "    \n",
    "    return cme_update\n",
    "\n",
    "\n",
    "def update_ensemble_conecmes(cme_params):\n",
    "    \"\"\"\n",
    "    Produce the list of updated conecmes.\n",
    "    \"\"\"\n",
    "    conecme_update = []\n",
    "    for i in range(cme_params.shape[0]):\n",
    "        conecme = H.ConeCME(t_launch=cme_params[i, 0]*u.s,\n",
    "                            longitude=cme_params[i, 1]*u.rad,\n",
    "                            latitude=cme_params[i, 2]*u.rad,\n",
    "                            width=cme_params[i, 3]*u.rad,\n",
    "                            v=cme_params[i, 4]*(u.km/u.s),\n",
    "                            thickness=(cme_params[i, 5]*u.km).to(u.solRad))\n",
    "        conecme_update.append(conecme)\n",
    "        \n",
    "    return conecme_update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already exist for CR2065\n"
     ]
    }
   ],
   "source": [
    "# Start up HUXt at a specified time.\n",
    "np.random.seed(192938)\n",
    "n_ensemble = 50\n",
    "obs_lon = -30*u.deg\n",
    "\n",
    "start_time = Time('2008-01-01T00:00:00')\n",
    "model = setup_huxt(start_time)\n",
    "\n",
    "# Initialise Earth directed CME. Coords in HEEQ, so need Earth Lat.\n",
    "ert = model.get_observer('EARTH')\n",
    "avg_ert_lat = np.mean(ert.lat.to(u.deg).value)\n",
    "cme_base = get_base_cme(v=1000, lon=0, lat=avg_ert_lat, width=45)\n",
    "\n",
    "# Perturb the base CME to get a \"pseudo-truth\", and solve\n",
    "cme_truth = perturb_cone_cme(cme_base)\n",
    "model.solve([cme_truth])\n",
    "cme_truth = model.cmes[0]\n",
    "truth_profile = compute_t_e_profile(obs_lon, cme_truth)\n",
    "\n",
    "# Compute estimate of observations (with some noise included)\n",
    "for kkk in range(5):\n",
    "    \n",
    "    obs_profile = compute_observed_cme_profile(obs_lon, cme_truth)\n",
    "    filename = \"SIR_HUXt_analysis_test_iter_{:02d}.hdf5\".format(kkk)\n",
    "\n",
    "    out_file = h5py.File(filename, 'w')\n",
    "    out_file.create_dataset('obs_lon', data=obs_lon.value)\n",
    "    out_file.create_dataset('n_ensemble', data=n_ensemble)\n",
    "    out_file.create_dataset('truth_profile', data=truth_profile.values)\n",
    "    out_file.create_dataset('obs_profile', data=obs_profile.values)\n",
    "    out_file.flush()\n",
    "\n",
    "    #Loop through the obs and do the SIR\n",
    "    for i, row in obs_profile.iterrows():\n",
    "        t_obs = row[0]\n",
    "        e_obs = row[1]\n",
    "\n",
    "        arrival_arr = np.zeros((n_ensemble))*np.NaN\n",
    "        lkhd_arr = np.zeros((n_ensemble))*np.NaN\n",
    "        all_cme_params = np.zeros((n_ensemble, 6))*np.NaN\n",
    "\n",
    "        for j in range(n_ensemble):\n",
    "\n",
    "            if i == 0:\n",
    "                cme_ens = perturb_cone_cme(cme_base)\n",
    "            else:\n",
    "                cme_ens = next_cone_cmes[j]\n",
    "\n",
    "            # Solve huxt with the perturbed CME\n",
    "            model.solve([cme_ens])\n",
    "            cme_ens = model.cmes[0]          \n",
    "\n",
    "            # Stash this CMEs parameters and arrival time\n",
    "            arrival_arr[j] = cme_ens.earth_arrival_time.jd\n",
    "            all_cme_params[j,:] = get_cme_params_for_sir(cme_ens)\n",
    "\n",
    "            # Get the time elon profile \n",
    "            model_profile = compute_t_e_profile(obs_lon, cme_ens)\n",
    "            # Compute the likelihood of the obs\n",
    "            lkhd_arr[j] = compute_observation_likelihood(t_obs, e_obs, model_profile)\n",
    "\n",
    "            # Append profile to all ensemble profiles\n",
    "            if j == 0:\n",
    "                all_profiles = model_profile.loc[:,['time', 'el']].copy()\n",
    "                all_profiles.rename(columns={\"el\": \"el_{:02d}\".format(j)}, inplace=True)\n",
    "            else:\n",
    "                all_profiles['el_{:02d}'.format(j)] = model_profile['el']\n",
    "\n",
    "        # Now compute the member weights\n",
    "        weights = lkhd_arr / np.nansum(lkhd_arr)\n",
    "        # Resample the CMEs and get updated list of Cone CMEs\n",
    "        next_cme_params = cme_kde_resample_with_weights(all_cme_params, weights)\n",
    "        next_cone_cmes = update_ensemble_conecmes(next_cme_params)\n",
    "\n",
    "        # Save the data\n",
    "        analysisgrp = out_file.create_group('analysis_{:02d}'.format(i))\n",
    "        analysisgrp.create_dataset('t_obs', data=t_obs)\n",
    "        analysisgrp.create_dataset('el_obs', data=e_obs)\n",
    "        analysisgrp.create_dataset('cme_params', data=all_cme_params)\n",
    "        analysisgrp.create_dataset('ensemble_profiles', data=all_profiles.values)\n",
    "        analysisgrp.create_dataset('likelihood', data=lkhd_arr)\n",
    "        analysisgrp.create_dataset('weights', data=weights)\n",
    "        out_file.flush()\n",
    "\n",
    "    out_file.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Not a location id (invalid object ID)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-378a1bd8c4d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0manalysis_keys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mout_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"_\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m\u001b[1;34m'analysis'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0manalysis_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\_collections_abc.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 720\u001b[1;33m         \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mapping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m \u001b[0mKeysView\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\group.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    407\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m         \u001b[1;34m\"\"\" Iterate over member names \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\h5g.pyx\u001b[0m in \u001b[0;36mh5py.h5g.GroupID.__iter__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5g.pyx\u001b[0m in \u001b[0;36mh5py.h5g.GroupID.__iter__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5g.pyx\u001b[0m in \u001b[0;36mh5py.h5g.GroupIter.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5g.pyx\u001b[0m in \u001b[0;36mh5py.h5g.GroupID.get_num_objs\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Not a location id (invalid object ID)"
     ]
    }
   ],
   "source": [
    "analysis_keys = []\n",
    "for k in out_file.keys():\n",
    "    if k.split(\"_\")[0] =='analysis':\n",
    "        analysis_keys.append(k)\n",
    "\n",
    "for key in analysis_keys:\n",
    "    fig, ax = plt.subplots()\n",
    "    path = key + \"/ensemble_profiles\"\n",
    "    data = out_file[path][()]\n",
    "    ax.plot(data[:,0], data[:,1:], 'k-')\n",
    "\n",
    "    data = out_file['truth_profile'][()]\n",
    "    truth_profile = pd.DataFrame(data, columns=['time', 'el', 'r', 'lon'])\n",
    "    ax.plot(truth_profile['time'], truth_profile['el'],'b-')\n",
    "\n",
    "    data = out_file['obs_profile'][()]\n",
    "    obs_profile = pd.DataFrame(data, columns=['time', 'el'])\n",
    "    ax.plot(obs_profile['time'], obs_profile['el'],'r.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
